{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ba86355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import DateSet, numpy2gpu,load_split_data\n",
    "from utils import gen_tests_of_deblur,x_post_sample_modify,prior_mean_cov\n",
    "from utils import sample_cumulated_sum\n",
    "from function import get_x_ml,x_post_sample_modify\n",
    "from cvae_model.visualize import printdict # 更改\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1bdd32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma_range: [0.01, 0.03, 0.05]\n",
      "noise_num: 3\n",
      "M_samples_per_para: 20000\n",
      "x_dim: 50\n",
      "data_dim: 50\n",
      "sigma_prior: 0.1\n",
      "    H: (50, 50)\n",
      "data_file_prefix: data\\signal_denoise_mu3_0.01_0.03_M20000\n",
      "---------------------------------------------------------------------------\n",
      "number of samples: 3*  20000  (noise_num*M_samples_per_para)\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "%run cvae_model/problem_setting.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2fab814-37e4-4127-872c-4a2667a8638f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d5fed0f-2755-4f94-911a-1d69c124fced",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "\n",
    "    def __init__(self, feature,classes,encoder_layer_sizes, latent_size, decoder_layer_sizes,\n",
    "                 conditional=True, num_labels=0):\n",
    "        super().__init__()\n",
    "\n",
    "        if conditional:\n",
    "            assert num_labels > 0\n",
    "\n",
    "        assert type(encoder_layer_sizes) == list\n",
    "        # assert type(layer_sizes) == list\n",
    "        assert type(latent_size) == int\n",
    "        assert type(decoder_layer_sizes) == list\n",
    "\n",
    "        self.latent_size = latent_size\n",
    "        self.net = NN(feature,classes)\n",
    "        self.encoder = Encoder(\n",
    "            encoder_layer_sizes, latent_size, conditional, num_labels)\n",
    "        self.decoder = Decoder(\n",
    "            decoder_layer_sizes, latent_size, conditional, num_labels)\n",
    "\n",
    "    def forward(self, x, unkown, data):\n",
    "        data_size = data.shape[1]\n",
    "        data = self.net(data)\n",
    "        class_data = torch.max(data, 1).indices.float().reshape(-1, 1)\n",
    "        class_data = class_data @ torch.ones(1,data_size).to(device)\n",
    "        c = torch.cat([unkown, class_data], dim=1)\n",
    "        means, log_var = self.encoder(x, c)\n",
    "        z = self.reparameterize(means, log_var)\n",
    "        recon_x = self.decoder(z, c)\n",
    "\n",
    "        return recon_x, z, data, class_data[0]\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "\n",
    "        return mu + eps * std\n",
    "\n",
    "    def inference(self, z, unkown, data):\n",
    "        data_size = data.shape[1]\n",
    "        data = self.net(data)\n",
    "        class_data = torch.max(data, 1).indices.float().reshape(-1, 1)\n",
    "        class_data = class_data @ torch.ones(1,data_size).to(device)\n",
    "        c = torch.cat([unkown, class_data], dim=1)\n",
    "        recon_x = self.decoder(z, c)\n",
    "        return recon_x,data,class_data\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, layer_sizes, latent_size, conditional, num_labels):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.conditional = conditional\n",
    "        if self.conditional:\n",
    "            layer_sizes[0] += num_labels\n",
    "\n",
    "        self.MLP = nn.Sequential()\n",
    "\n",
    "        for i, (in_size, out_size) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "            self.MLP.add_module(name=\"L{:d}\".format(i), module=nn.Linear(in_size, out_size))\n",
    "            if i + 2 < len(layer_sizes):\n",
    "                self.MLP.add_module(name=\"A{:d}\".format(i), module=nn.LeakyReLU())\n",
    "\n",
    "        self.linear_means = nn.Sequential(nn.Linear(layer_sizes[-1], latent_size),nn.BatchNorm1d(latent_size))\n",
    "        # self.linear_means = nn.Linear(layer_sizes[-1], latent_size)\n",
    "        self.linear_log_var = nn.Linear(layer_sizes[-1], latent_size)\n",
    "\n",
    "    def forward(self, x, c=None):\n",
    "\n",
    "        if self.conditional:\n",
    "            x = torch.cat((x, c), dim=-1)\n",
    "\n",
    "        x = self.MLP(x)\n",
    "\n",
    "        means = self.linear_means(x)\n",
    "        log_vars = self.linear_log_var(x)\n",
    "\n",
    "        return means, log_vars\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, layer_sizes, latent_size, conditional, num_labels):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.MLP = nn.Sequential()\n",
    "        self.conditional = conditional\n",
    "        if self.conditional:\n",
    "            input_size = latent_size + num_labels\n",
    "        else:\n",
    "            input_size = latent_size\n",
    "\n",
    "        for i, (in_size, out_size) in enumerate(zip([input_size] + layer_sizes[:-1], layer_sizes)):\n",
    "            self.MLP.add_module(name=\"L{:d}\".format(i), module=nn.Linear(in_size, out_size))\n",
    "            if i + 1 < len(layer_sizes):\n",
    "                self.MLP.add_module(name=\"A{:d}\".format(i), module=nn.LeakyReLU())\n",
    "                # self.MLP.add_module(name=\"B{:d}\".format(i), module=nn.BatchNorm1d(out_size))\n",
    "            # else:\n",
    "            #     self.MLP.add_module(name=\"Output\", module=nn.Softplus())#[batchsize,layer_sizes[-1]]\n",
    "\n",
    "    # 最后一层不可以是1了...只能是指定的类别\n",
    "    def forward(self, z, c):\n",
    "        if self.conditional:\n",
    "            z = torch.cat((z, c), dim=1)\n",
    "        x = self.MLP(z)\n",
    "        out = x\n",
    "        return out\n",
    "\n",
    "\n",
    "# class NN(nn.Module):\n",
    "#     def __init__(self, layer_size):\n",
    "#         super(NN, self).__init__()  # 调用父类的初始化函数\n",
    "#         assert type(layer_size) == list\n",
    "#         self.classMLP = nn.Sequential()\n",
    "#         for i, (in_size, out_size) in enumerate(zip(layer_size[:-1], layer_size[1:])):\n",
    "#             self.classMLP.add_module(name=\"L{:d}\".format(i), module=nn.Linear(in_size, out_size))\n",
    "#             if i + 2 < len(layer_size):\n",
    "#                 self.classMLP.add_module(name=\"A{:d}\".format(i), module=nn.LeakyReLU())\n",
    "\n",
    "#     def forward(self, data):\n",
    "#         data = self.classMLP(data)\n",
    "#         data = torch.nn.functional.softmax(data,dim=1)\n",
    "#         return data\n",
    "\n",
    "class Residual_block(nn.Module):\n",
    "    def __init__(self,in_features,out_features,hidden_features = 100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dense0 = nn.Linear(in_features,hidden_features)\n",
    "        self.dropout0 = nn.Dropout(p=0.2)\n",
    "\n",
    "        self.dense1 = nn.Linear(hidden_features,out_features)\n",
    "        self.dropout1 = nn.Dropout(p=0.2)\n",
    "        self.dense_skip = nn.Linear(in_features,out_features)\n",
    "\n",
    "    def forward(self,input):\n",
    "        l1 = nn.LeakyReLU()(self.dropout0(self.dense0(input)))\n",
    "        l2 = nn.LeakyReLU()(self.dropout1(self.dense1(l1)))\n",
    "        skip = nn.LeakyReLU()(self.dense_skip(input))\n",
    "        output = skip+l2\n",
    "        return output\n",
    "    \n",
    "class NN(nn.Module):\n",
    "    def __init__(self,feature,classs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.MLP = nn.Sequential(Residual_block(feature,feature),Residual_block(feature,feature),\n",
    "                        Residual_block(feature,feature),\n",
    "                        nn.Linear(feature,classs))\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "        \n",
    "    def forward(self,input):\n",
    "        output = self.MLP(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06270c44-4478-4919-b736-9dc134e8f170",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "\n",
    "    def __init__(self, layer_sizes, encoder_layer_sizes, latent_size, decoder_layer_sizes,\n",
    "                 conditional=True, num_labels=0):\n",
    "        super().__init__()\n",
    "\n",
    "        if conditional:\n",
    "            assert num_labels > 0\n",
    "\n",
    "        assert type(encoder_layer_sizes) == list\n",
    "        assert type(layer_sizes) == list\n",
    "        assert type(latent_size) == int\n",
    "        assert type(decoder_layer_sizes) == list\n",
    "\n",
    "        self.latent_size = latent_size\n",
    "        self.net = NN(layer_sizes)\n",
    "        self.encoder = Encoder(\n",
    "            encoder_layer_sizes, latent_size, conditional, num_labels)\n",
    "        self.decoder = Decoder(\n",
    "            decoder_layer_sizes, latent_size, conditional, num_labels)\n",
    "\n",
    "    def forward(self, x, unkown, data):\n",
    "        data_size = data.shape[1]\n",
    "        data = self.net(data)\n",
    "        class_data = torch.max(data, 1).indices.float().reshape(-1, 1)\n",
    "        class_data = class_data @ torch.ones(1,data_size).to(device)\n",
    "        c = torch.cat([unkown, class_data], dim=1)\n",
    "        means, log_var = self.encoder(x, c)\n",
    "        z = self.reparameterize(means, log_var)\n",
    "        recon_x = self.decoder(z, c)\n",
    "\n",
    "        return recon_x, z, data, class_data[0]\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "\n",
    "        return mu + eps * std\n",
    "\n",
    "    def inference(self, z, unkown, data):\n",
    "        data_size = data.shape[1]\n",
    "        data = self.net(data)\n",
    "        class_data = torch.max(data, 1).indices.float().reshape(-1, 1)\n",
    "        class_data = class_data @ torch.ones(1,data_size).to(device)\n",
    "        c = torch.cat([unkown, class_data], dim=1)\n",
    "        recon_x = self.decoder(z, c)\n",
    "        return recon_x,data,class_data\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, layer_sizes, latent_size, conditional, num_labels):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.conditional = conditional\n",
    "        if self.conditional:\n",
    "            layer_sizes[0] += num_labels\n",
    "\n",
    "        self.MLP = nn.Sequential()\n",
    "\n",
    "        for i, (in_size, out_size) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "            self.MLP.add_module(name=\"L{:d}\".format(i), module=nn.Linear(in_size, out_size))\n",
    "            if i + 2 < len(layer_sizes):\n",
    "                self.MLP.add_module(name=\"A{:d}\".format(i), module=nn.LeakyReLU())\n",
    "\n",
    "        # self.linear_means = nn.Sequential(nn.Linear(layer_sizes[-1], latent_size),nn.BatchNorm1d(latent_size))\n",
    "        self.linear_means = nn.Linear(layer_sizes[-1], latent_size)\n",
    "        self.linear_log_var = nn.Linear(layer_sizes[-1], latent_size)\n",
    "\n",
    "    def forward(self, x, c=None):\n",
    "\n",
    "        if self.conditional:\n",
    "            x = torch.cat((x, c), dim=-1)\n",
    "\n",
    "        x = self.MLP(x)\n",
    "\n",
    "        means = self.linear_means(x)\n",
    "        log_vars = self.linear_log_var(x)\n",
    "\n",
    "        return means, log_vars\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, layer_sizes, latent_size, conditional, num_labels):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.MLP = nn.Sequential()\n",
    "        self.conditional = conditional\n",
    "        if self.conditional:\n",
    "            input_size = latent_size + num_labels\n",
    "        else:\n",
    "            input_size = latent_size\n",
    "\n",
    "        for i, (in_size, out_size) in enumerate(zip([input_size] + layer_sizes[:-1], layer_sizes)):\n",
    "            self.MLP.add_module(name=\"L{:d}\".format(i), module=nn.Linear(in_size, out_size))\n",
    "            if i + 1 < len(layer_sizes):\n",
    "                self.MLP.add_module(name=\"A{:d}\".format(i), module=nn.LeakyReLU())\n",
    "                # self.MLP.add_module(name=\"B{:d}\".format(i), module=nn.BatchNorm1d(out_size))\n",
    "            # else:\n",
    "            #     self.MLP.add_module(name=\"Output\", module=nn.Softplus())#[batchsize,layer_sizes[-1]]\n",
    "\n",
    "    # 最后一层不可以是1了...只能是指定的类别\n",
    "    def forward(self, z, c):\n",
    "        if self.conditional:\n",
    "            z = torch.cat((z, c), dim=1)\n",
    "        x = self.MLP(z)\n",
    "        out = x\n",
    "        return out\n",
    "\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, layer_size):\n",
    "        super(NN, self).__init__()  # 调用父类的初始化函数\n",
    "        assert type(layer_size) == list\n",
    "        self.classMLP = nn.Sequential()\n",
    "        for i, (in_size, out_size) in enumerate(zip(layer_size[:-1], layer_size[1:])):\n",
    "            self.classMLP.add_module(name=\"L{:d}\".format(i), module=nn.Linear(in_size, out_size))\n",
    "            if i + 2 < len(layer_size):\n",
    "                self.classMLP.add_module(name=\"A{:d}\".format(i), module=nn.LeakyReLU())\n",
    "\n",
    "    def forward(self, data):\n",
    "        data = self.classMLP(data)\n",
    "        data = torch.nn.functional.softmax(data,dim=1)\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccde5b7",
   "metadata": {},
   "source": [
    "# load model and simulate test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e1ceed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0524_bn_cvae_class3_lr1e-05_accuracy0.53125\n",
      "epochs: 40\n",
      "batch_size: 64\n",
      "layer_sizes: [50, 100, 200, 400, 200, 100, 50, 20, 3]\n",
      "encoder_layer_sizes: [101, 200, 100, 50, 30, 10]\n",
      "decoder_layer_sizes: [200, 100, 50, 30, 12, 5, 1]\n",
      "latent_size: 2\n",
      "print_every: 5000\n",
      "fig_root: figs\n",
      "conditional: True\n",
      "sigma_range: [0.01, 0.03, 0.05]\n",
      "noise_num: 3\n",
      "M_samples_per_para: 20000\n",
      "x_dim: 50\n",
      "data_dim: 50\n",
      "sigma_prior: 0.1\n",
      "    H: (50, 50)\n",
      "data_file_prefix: data/signal_denoise_mu3_0.01_0.03_M20000\n"
     ]
    }
   ],
   "source": [
    "model_file_name = '0524_bn_cvae_class3_lr1e-05_accuracy0.53125' #??\n",
    "\n",
    "args = np.load(os.path.join('saved_model',model_file_name)+\"_args.npy\",allow_pickle=True).item()\n",
    "hypers = np.load(os.path.join('saved_model',model_file_name)+\"_hypers.npy\",allow_pickle=True).item() ##############??\n",
    "\n",
    "print(model_file_name)\n",
    "printdict(args),printdict(hypers)\n",
    "\n",
    "cvae = torch.load(os.path.join('saved_model',model_file_name)+\".pth\").to('cpu') #??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5e51aa-7c88-49e5-b1a3-2051529e22fe",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fca047fa-0d4d-40fe-84b7-f996c99bd51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dim = hypers['x_dim']\n",
    "n = 1 # 一共有10个数据\n",
    "C = np.identity(x_dim) * (hypers['sigma_prior']**2)\n",
    "x = np.random.multivariate_normal([0] * x_dim, C,n) # [M,x_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e224b1-0354-4957-bf0a-dd465d9599b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_0_gpu,_,classes = cvae.inference(z, torch.tensor(x).float(),torch.tensor(y_j.T).float().reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1070756f-1bd3-46c3-a777-c43a350af19c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_0.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0882104d-3d66-4d37-baf8-9d0999689a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1000fb8-673f-4e58-80b2-38d36f996503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89e26097-02e1-40d8-b363-7663edd41ecf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 产生真实数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d898b62e-787c-4ce7-8b07-0f408c2bab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv(r'D:\\LiulanqiDownload\\jupyter code\\2021论文\\signal_denoise\\x.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d3f5ab5-4041-4b44-9d85-4dff004715d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d0c4faf-73c4-4336-abe7-d9bacf1d38bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6b017b7c-7485-421e-8ad1-f66e9dab96f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = pd.DataFrame(np.mean(x,0))\n",
    "truth.to_csv('gt.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133389f2-7391-45ee-bf5f-c429051ba641",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 没有标准化的版本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9d2101-3290-4e04-ac50-ee8f20a6ce68",
   "metadata": {},
   "source": [
    "1.从x的先验分布中抽样产生了20个无噪音干扰的信号值X；2.针对每一个信号值，分别使用两种方法进行对比。我们的方法与AGEM方法抽样的数目为1000次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e1b5ac8-120c-4471-9bec-4cf9d6800c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dim = hypers['x_dim']\n",
    "n = 1 # 一共有10个数据\n",
    "C = np.identity(x_dim) * (hypers['sigma_prior']**2)\n",
    "x = np.random.multivariate_normal([0] * x_dim, C,n) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4457f12b-e13f-483c-a214-77d6839dca2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0226]], grad_fn=<AddmmBackward0>),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0.]]))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_0_gpu,classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c9a32af-2691-40c6-95a7-bf16d1e6f8a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.20154257 -0.05825211  0.04590706  0.11341348  0.10579308 -0.12378718\n",
      " -0.07778999 -0.03277799 -0.02742185  0.14856825  0.0586014  -0.07712998\n",
      " -0.01236319 -0.05780515  0.203189    0.040509    0.18596863  0.04839848\n",
      "  0.01667894  0.00705331  0.18352318  0.15004759 -0.04799447 -0.15526241\n",
      " -0.18560074  0.05501587  0.08466795 -0.0141447   0.10880269  0.08078488\n",
      "  0.06253116 -0.09945541 -0.24109735 -0.16787838  0.15563407 -0.0955862\n",
      "  0.08019653  0.03398956 -0.15758084  0.07672417 -0.04449069 -0.14740187\n",
      "  0.13174537 -0.13474907  0.01276222  0.05234089  0.06342796  0.02939549\n",
      " -0.07206927 -0.16781844]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x105 and 102x200)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# x_0 = np.random.multivariate_normal([0] * x_dim, C,n) .T\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(x_0\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[1;32m---> 11\u001b[0m theta_0_gpu,_,classes \u001b[38;5;241m=\u001b[39m \u001b[43mcvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# if theta_0_gpu <=0.02:\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(theta_0_gpu,classes)\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mVAE.inference\u001b[1;34m(self, z, unkown, data)\u001b[0m\n\u001b[0;32m     44\u001b[0m class_data \u001b[38;5;241m=\u001b[39m class_data \u001b[38;5;241m@\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m1\u001b[39m,data_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     45\u001b[0m c \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([unkown, class_data], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m recon_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m recon_x,data,class_data\n",
      "File \u001b[1;32mD:\\AAAAADownload\\anaconda\\envs\\py3.8\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[1;34m(self, z, c)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconditional:\n\u001b[0;32m    108\u001b[0m     z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((z, c), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 109\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m out \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mD:\\AAAAADownload\\anaconda\\envs\\py3.8\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\AAAAADownload\\anaconda\\envs\\py3.8\\lib\\site-packages\\torch\\nn\\modules\\container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mD:\\AAAAADownload\\anaconda\\envs\\py3.8\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\AAAAADownload\\anaconda\\envs\\py3.8\\lib\\site-packages\\torch\\nn\\modules\\linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x105 and 102x200)"
     ]
    }
   ],
   "source": [
    "# [M,x_dim]\n",
    "\n",
    "for i in range(10):\n",
    "    # x = np.random.multivariate_normal([0] * x_dim, C,n)\n",
    "    b = np.random.randn(1,50) * 0.05\n",
    "    z = torch.randn([1, 5])\n",
    "    y = x + b\n",
    "    x_0 =  get_x_ml(y,hypers)\n",
    "    # x_0 = np.random.multivariate_normal([0] * x_dim, C,n) .T\n",
    "    theta_0_gpu,_,classes = cvae.inference(z, torch.tensor(x_0.T).float(),torch.tensor(y.T).float().reshape(1,-1))\n",
    "    \n",
    "    # if theta_0_gpu <=0.02:\n",
    "    print(theta_0_gpu,classes)\n",
    " \n",
    "    # if classes[0][0]==2:\n",
    "    #     print(theta_0_gpu)\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba30cb5d-54a8-4b28-a56e-1adb65fb30fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_1 = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cfe8017-71b6-4bb3-97f5-d6def5e237b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2 = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1029156b-d768-49dc-84e6-be2ca4148a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('y.npz', y0=y_0, y1=y_1,y2=y_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "19f80695-5694-49e1-a38f-f6922e15f675",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.load('data/y.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d41244e4-5f38-4d73-8168-71c102d8e308",
   "metadata": {},
   "outputs": [],
   "source": [
    "y =y['y2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6d138d0f-ae7f-4c6a-b2d9-1df7fbb216aa",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.98s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.91s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.96s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.53s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.22s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.20s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.70s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.54s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:10<00:00, 10.94s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:10<00:00, 10.36s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.31s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.87s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:10<00:00, 10.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "[CVAE_within_Gibbd] noise_gt: 0.05 | rmse 0.0520, noise_est: 0.0410(0.001416)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:10<00:00, 10.10s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:10<00:00, 10.03s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:10<00:00, 10.06s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:10<00:00, 10.08s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:10<00:00, 10.17s/it]\n",
      "  0%|                                                                                            | 0/1 [00:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [86]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# sample theta\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     c \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((torch\u001b[38;5;241m.\u001b[39mtensor(x_0)\u001b[38;5;241m.\u001b[39mfloat(),torch\u001b[38;5;241m.\u001b[39mtensor(y_j\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mfloat()),dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 37\u001b[0m     theta_0_gpu,_,classes \u001b[38;5;241m=\u001b[39m \u001b[43mcvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_j\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#更改\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     theta_0 \u001b[38;5;241m=\u001b[39m theta_0_gpu\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     39\u001b[0m   \u001b[38;5;66;03m# 因为之前在训练网络的时候，训练数据的时候我们先标准化数据了。这个时候需要标准化回来。乘以的也是原本训练时候的均值与方差\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# sample x\u001b[39;00m\n",
      "File \u001b[1;32mD:\\LiulanqiDownload\\jupyter code\\2021论文\\signal_denoise\\519\\train_cvae\\cvae_models.py:52\u001b[0m, in \u001b[0;36mVAE.inference\u001b[1;34m(self, z, unkown, data)\u001b[0m\n\u001b[0;32m     50\u001b[0m class_data \u001b[38;5;241m=\u001b[39m class_data \u001b[38;5;241m@\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m1\u001b[39m,data_size)\n\u001b[0;32m     51\u001b[0m c \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([unkown, class_data], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 52\u001b[0m recon_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m recon_x,data,class_data\n",
      "File \u001b[1;32mD:\\AAAAADownload\\anaconda\\envs\\py3.8\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\LiulanqiDownload\\jupyter code\\2021论文\\signal_denoise\\519\\train_cvae\\cvae_models.py:118\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[1;34m(self, z, c)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconditional:\n\u001b[0;32m    117\u001b[0m     z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((z, c), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 118\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m out \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mD:\\AAAAADownload\\anaconda\\envs\\py3.8\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\AAAAADownload\\anaconda\\envs\\py3.8\\lib\\site-packages\\torch\\nn\\modules\\container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mD:\\AAAAADownload\\anaconda\\envs\\py3.8\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\AAAAADownload\\anaconda\\envs\\py3.8\\lib\\site-packages\\torch\\nn\\modules\\linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    for sigma_true in [0.05]: #\n",
    "        hypers['sigma_noise'] = sigma_true\n",
    "        sigma_noise = hypers['sigma_noise']\n",
    "        # # np.random.seed(0)\n",
    "        Hx = x\n",
    "        b = np.random.randn(n,50) * sigma_noise\n",
    "        y = Hx + b\n",
    "\n",
    "        THETA_mean = np.zeros([n,1])\n",
    "        THETA_var = np.zeros([n,1])\n",
    "        RMSE = np.zeros([n,1])\n",
    "    #     X_var = np.zeros([n,1])\n",
    "\n",
    "    #     init=np.zeros([n,1])\n",
    "\n",
    "        for j in tqdm(range(n)):    \n",
    "            # def Gibbs_sample(N,use_sample_size,x_truth,y,hypers,show_step=200,theta_truth=None):\n",
    "            N = 1000\n",
    "            use_sample_size = 500\n",
    "            x_truth = x[j].reshape(-1,1) # 50,n\n",
    "            theta_truth = np.zeros(1)#??\n",
    "            m = hypers['x_dim']\n",
    "            y_j = y[j]\n",
    "\n",
    "            x_0 =  get_x_ml(y_j,hypers)\n",
    "            x_init_value = x_0\n",
    "    #         init[j] = x_init_value\n",
    "\n",
    "            x_sum,x_square_sum = np.zeros_like(x_truth),np.zeros_like(x_truth)\n",
    "            theta_sum,theta_square_sum = np.zeros_like(theta_truth),np.zeros_like(theta_truth)  \n",
    "\n",
    "            z = torch.randn([1, args['latent_size']])\n",
    "            for i in range(N):\n",
    "                # sample theta\n",
    "                c = torch.cat((torch.tensor(x_0).float(),torch.tensor(y_j.reshape(-1,1)).float()),dim=0).T.to(device)\n",
    "                theta_0_gpu,_,classes = cvae.inference(z, torch.tensor(x_0.T).float(),torch.tensor(y_j.T).float().reshape(1,-1)) #更改\n",
    "                theta_0 = theta_0_gpu.detach().cpu().numpy()[0]\n",
    "              # 因为之前在训练网络的时候，训练数据的时候我们先标准化数据了。这个时候需要标准化回来。乘以的也是原本训练时候的均值与方差\n",
    "                # sample x\n",
    "                sigma = theta_0\n",
    "                t.append(sigma)\n",
    "\n",
    "                x_0,_,_ = x_post_sample_modify(m,hypers['sigma_prior'],sigma,y_j.reshape(-1,1))\n",
    "    #             X.append(np.sqrt(((x_0 - x_truth)**2).sum()/50))\n",
    "\n",
    "                if i>(N-use_sample_size-1):\n",
    "                    theta_sum,theta_square_sum = sample_cumulated_sum(theta_sum,theta_square_sum,theta_0)\n",
    "                    x_sum,x_square_sum = sample_cumulated_sum(x_sum,x_square_sum,x_0)   \n",
    "\n",
    "            x_mean = x_sum/use_sample_size\n",
    "            x_var = x_square_sum/use_sample_size-x_mean**2\n",
    "\n",
    "            theta_mean = theta_sum/use_sample_size\n",
    "            theta_var = theta_square_sum/use_sample_size-theta_mean**2\n",
    "            rmse = np.sqrt(((x_mean - x_truth)**2).sum()/50)\n",
    "\n",
    "            THETA_mean[j] = theta_mean\n",
    "            THETA_var[j] = theta_var\n",
    "            RMSE[j] = rmse\n",
    "    #         X_var[j] = x_var\n",
    "    #         X_mean.append(x_mean)\n",
    "    #         RMSE_total.append(rmse)\n",
    "        \n",
    "        noise_mean = THETA_mean.mean()\n",
    "        noise_std = np.sqrt(THETA_var).mean()\n",
    "    #     signal_var = X_var.mean()\n",
    "        rmse_mean = RMSE.mean()\n",
    "        if noise_mean>=0.04:\n",
    "            print(classes)\n",
    "            print('[CVAE_within_Gibbd] noise_gt: %.2f | rmse %.4f, noise_est: %.4f(%.6f)' % (sigma_true, rmse_mean, noise_mean,noise_std))\n",
    "            np.savez('y_0.05.npz', y)\n",
    "            break\n",
    "        break\n",
    "    break\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    # 每一个参数测试20次，每次固定参数以后，在第2次的时候，得到的x_mean将和对应的x_truth求RMSE。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "43397078-4a96-4c03-920b-a7524989d7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE_total = []\n",
    "# X_mean = []\n",
    "x_dim = hypers['x_dim']\n",
    "n = 1 # 一共有10个数据\n",
    "# np.random.seed(1)\n",
    "C = np.identity(x_dim) * (hypers['sigma_prior']**2)\n",
    "x = np.random.multivariate_normal([0] * x_dim, C,n) # [M,x_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b918aa83-7c3e-4d32-9ca4-f0db619c8cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_truth = x\n",
    "np.save('data/x_truth',x_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5e55092a-3a17-4e6f-aa37-8432ef02e79d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▊                                                                                | 1/30 [00:06<02:55,  6.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "[CVAE_within_Gibbd] noise_gt: 0.01 | rmse 0.0098, noise_est: 0.0232(0.000549)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████▌                                                                             | 2/30 [00:12<02:48,  6.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "[CVAE_within_Gibbd] noise_gt: 0.01 | rmse 0.0107, noise_est: 0.0231(0.000670)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████▎                                                                          | 3/30 [00:19<03:05,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "[CVAE_within_Gibbd] noise_gt: 0.01 | rmse 0.0121, noise_est: 0.0262(0.000756)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|███████████                                                                        | 4/30 [00:27<03:08,  7.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "[CVAE_within_Gibbd] noise_gt: 0.01 | rmse 0.0109, noise_est: 0.0217(0.000579)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█████████████▊                                                                     | 5/30 [00:36<03:10,  7.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "[CVAE_within_Gibbd] noise_gt: 0.01 | rmse 0.0094, noise_est: 0.0180(0.000526)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▌                                                                  | 6/30 [00:44<03:09,  7.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "[CVAE_within_Gibbd] noise_gt: 0.01 | rmse 0.0102, noise_est: 0.0204(0.000580)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|███████████████████▎                                                               | 7/30 [00:53<03:07,  8.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "[CVAE_within_Gibbd] noise_gt: 0.01 | rmse 0.0086, noise_est: 0.0210(0.000842)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██████████████████████▏                                                            | 8/30 [01:02<03:06,  8.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "[CVAE_within_Gibbd] noise_gt: 0.01 | rmse 0.0108, noise_est: 0.0247(0.000650)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████▉                                                          | 9/30 [01:10<02:55,  8.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "[CVAE_within_Gibbd] noise_gt: 0.01 | rmse 0.0116, noise_est: 0.0234(0.000756)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███████████████████████████▎                                                      | 10/30 [01:18<02:46,  8.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "[CVAE_within_Gibbd] noise_gt: 0.01 | rmse 0.0104, noise_est: 0.0252(0.001230)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|██████████████████████████████                                                    | 11/30 [01:27<02:41,  8.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "[CVAE_within_Gibbd] noise_gt: 0.01 | rmse 0.0093, noise_est: 0.0245(0.000647)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████▊                                                 | 12/30 [01:36<02:33,  8.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "[CVAE_within_Gibbd] noise_gt: 0.01 | rmse 0.0112, noise_est: 0.0207(0.001043)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|███████████████████████████████████▌                                              | 13/30 [01:44<02:23,  8.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "[CVAE_within_Gibbd] noise_gt: 0.01 | rmse 0.0115, noise_est: 0.0204(0.000480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|██████████████████████████████████████▎                                           | 14/30 [01:52<02:12,  8.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "[CVAE_within_Gibbd] noise_gt: 0.01 | rmse 0.0100, noise_est: 0.0245(0.000700)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████                                         | 15/30 [01:59<02:01,  8.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "[CVAE_within_Gibbd] noise_gt: 0.01 | rmse 0.0112, noise_est: 0.0252(0.000745)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|███████████████████████████████████████████▋                                      | 16/30 [02:07<01:52,  8.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "[CVAE_within_Gibbd] noise_gt: 0.01 | rmse 0.0117, noise_est: 0.0240(0.000596)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|██████████████████████████████████████████████▍                                   | 17/30 [02:15<01:44,  8.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "[CVAE_within_Gibbd] noise_gt: 0.01 | rmse 0.0111, noise_est: 0.0294(0.000851)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████▏                                | 18/30 [02:24<01:37,  8.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "[CVAE_within_Gibbd] noise_gt: 0.01 | rmse 0.0093, noise_est: 0.0221(0.000618)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|███████████████████████████████████████████████████▉                              | 19/30 [02:32<01:31,  8.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "[CVAE_within_Gibbd] noise_gt: 0.01 | rmse 0.0129, noise_est: 0.0227(0.000983)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████████████████████████████████████████████████████▋                           | 20/30 [02:41<01:23,  8.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "[CVAE_within_Gibbd] noise_gt: 0.01 | rmse 0.0097, noise_est: 0.0237(0.000721)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████▍                        | 21/30 [02:50<01:16,  8.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "[CVAE_within_Gibbd] noise_gt: 0.01 | rmse 0.0115, noise_est: 0.0211(0.000586)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|████████████████████████████████████████████████████████████▏                     | 22/30 [02:58<01:06,  8.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "[CVAE_within_Gibbd] noise_gt: 0.01 | rmse 0.0122, noise_est: 0.0235(0.000676)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|██████████████████████████████████████████████████████████████▊                   | 23/30 [03:06<00:57,  8.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "[CVAE_within_Gibbd] noise_gt: 0.01 | rmse 0.0121, noise_est: 0.0231(0.000817)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████████▌                | 24/30 [03:14<00:49,  8.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "[CVAE_within_Gibbd] noise_gt: 0.01 | rmse 0.0120, noise_est: 0.0245(0.000639)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████████████████████████████████████████████████████████████████▎             | 25/30 [03:22<00:40,  8.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "[CVAE_within_Gibbd] noise_gt: 0.01 | rmse 0.0116, noise_est: 0.0236(0.000616)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|███████████████████████████████████████████████████████████████████████           | 26/30 [03:30<00:33,  8.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "[CVAE_within_Gibbd] noise_gt: 0.01 | rmse 0.0123, noise_est: 0.0246(0.000801)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████▊        | 27/30 [03:39<00:25,  8.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "[CVAE_within_Gibbd] noise_gt: 0.01 | rmse 0.0117, noise_est: 0.0220(0.000618)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|████████████████████████████████████████████████████████████████████████████▌     | 28/30 [03:47<00:16,  8.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "[CVAE_within_Gibbd] noise_gt: 0.01 | rmse 0.0106, noise_est: 0.0270(0.000685)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|███████████████████████████████████████████████████████████████████████████████▎  | 29/30 [03:55<00:08,  8.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "[CVAE_within_Gibbd] noise_gt: 0.01 | rmse 0.0099, noise_est: 0.0224(0.000684)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [04:03<00:00,  8.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "[CVAE_within_Gibbd] noise_gt: 0.01 | rmse 0.0118, noise_est: 0.0208(0.000642)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for test_num in tqdm(range(30)):\n",
    "    t = []\n",
    "    for sigma_true in [0.01]: #\n",
    "        hypers['sigma_noise'] = sigma_true\n",
    "        sigma_noise = hypers['sigma_noise']\n",
    "        Hx = x\n",
    "        b = np.random.randn(n,50) * sigma_noise\n",
    "        y = Hx + b\n",
    "        \n",
    "\n",
    "\n",
    "        THETA_mean = np.zeros([n,1])\n",
    "        THETA_var = np.zeros([n,1])\n",
    "        RMSE = np.zeros([n,1])\n",
    "\n",
    "        # np.random.seed(7)\n",
    "        # torch.manual_seed(7)\n",
    "        for j in range(n):    \n",
    "            # def Gibbs_sample(N,use_sample_size,x_truth,y,hypers,show_step=200,theta_truth=None):\n",
    "            N = 10000\n",
    "            use_sample_size = 5000\n",
    "            x_truth = x[j].reshape(-1,1) # 50,n\n",
    "            theta_truth = np.zeros(1)#??\n",
    "            m = hypers['x_dim']\n",
    "            y_j = y[j]\n",
    "\n",
    "            x_0 =  get_x_ml(y_j,hypers)\n",
    "            x_init_value = x_0\n",
    "    #         init[j] = x_init_value\n",
    "\n",
    "            x_sum,x_square_sum = np.zeros_like(x_truth),np.zeros_like(x_truth)\n",
    "            theta_sum,theta_square_sum = np.zeros_like(theta_truth),np.zeros_like(theta_truth)  \n",
    "\n",
    "            z = torch.randn([1, args['latent_size']])\n",
    "            for i in range(N):\n",
    "                # sample theta\n",
    "                c = torch.cat((torch.tensor(x_0).float(),torch.tensor(y_j.reshape(-1,1)).float()),dim=0).T.to(device)\n",
    "                theta_0_gpu,_,classes = cvae.inference(z, torch.tensor(x_0.T).float(),torch.tensor(y_j.T).float().reshape(1,-1)) #更改\n",
    "                theta_0 = theta_0_gpu.detach().cpu().numpy()[0]\n",
    "              # 因为之前在训练网络的时候，训练数据的时候我们先标准化数据了。这个时候需要标准化回来。乘以的也是原本训练时候的均值与方差\n",
    "                # sample x\n",
    "                sigma = theta_0\n",
    "                t.append(sigma)\n",
    "\n",
    "                x_0,_,_ = x_post_sample_modify(m,hypers['sigma_prior'],sigma,y_j.reshape(-1,1))\n",
    "    #             X.append(np.sqrt(((x_0 - x_truth)**2).sum()/50))\n",
    "\n",
    "                if i>(N-use_sample_size-1):\n",
    "                    theta_sum,theta_square_sum = sample_cumulated_sum(theta_sum,theta_square_sum,theta_0)\n",
    "                    x_sum,x_square_sum = sample_cumulated_sum(x_sum,x_square_sum,x_0)   \n",
    "\n",
    "            x_mean = x_sum/use_sample_size\n",
    "            x_var = x_square_sum/use_sample_size-x_mean**2\n",
    "\n",
    "            theta_mean = theta_sum/use_sample_size\n",
    "            theta_var = theta_square_sum/use_sample_size-theta_mean**2\n",
    "            rmse = np.sqrt(((x_mean - x_truth)**2).sum()/50)\n",
    "\n",
    "            THETA_mean[j] = theta_mean\n",
    "            THETA_var[j] = theta_var\n",
    "            RMSE[j] = rmse\n",
    "\n",
    "        print(classes[0][0])\n",
    "        noise_mean = THETA_mean.mean()\n",
    "        noise_std = np.sqrt(THETA_var).mean()\n",
    "        rmse_mean = RMSE.mean()\n",
    "\n",
    "        print('[CVAE_within_Gibbd] noise_gt: %.2f | rmse %.4f, noise_est: %.4f(%.6f)' % (\n",
    "        sigma_true, rmse_mean, noise_mean,noise_std\n",
    "    ))\n",
    "        if noise_mean<=0.02:\n",
    "            np.save(f'data/y{test_num}_{sigma_noise}.npy',y)\n",
    "\n",
    "\n",
    "\n",
    "    # 每一个参数测试20次，每次固定参数以后，在第2次的时候，得到的x_mean将和对应的x_truth求RMSE。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9099618-498d-4374-919f-53b5c2a21a8a",
   "metadata": {},
   "source": [
    "# CVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c28dfac-e9a1-40d7-8a1e-12b6205e88ac",
   "metadata": {},
   "source": [
    "resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "18c14258-b6e7-4f74-a1f9-b91c467d7032",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2032c7ea-e2a3-4271-adbb-b9dd4f00367a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.multivariate_normal([0] * x_dim, C,n).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5ec04dc8-9c39-4f91-9102-207271661c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0178]], grad_fn=<AddmmBackward0>) tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "# [M,x_dim]\n",
    "# x_dim = hypers['x_dim']\n",
    "# n = 1 # 一共有10个数据\n",
    "# C = np.identity(x_dim) * (hypers['sigma_prior']**2)\n",
    "# x = np.random.multivariate_normal([0] * x_dim, C,n)\n",
    "x = np.load('data/524_x.npy').T\n",
    "\n",
    "for i in range(1000):\n",
    "    # x = np.random.multivariate_normal([0] * x_dim, C,n)\n",
    "    b = np.random.randn(1,50) * 0.01\n",
    "    \n",
    "    y = x + b\n",
    "    x_0 =  get_x_ml(y,hypers)\n",
    "    z = torch.randn([1, 2])\n",
    "    # x_0 = np.random.multivariate_normal([0] * x_dim, C,n) .T\n",
    "\n",
    "    theta_0_gpu,_,classes = cvae.inference(z, torch.tensor(x_0.T).float(),torch.tensor(y.T).float().reshape(1,-1))\n",
    "    # print(theta_0_gpu,classes[0][0])\n",
    "    if theta_0_gpu <=0.02:\n",
    "        print(theta_0_gpu,classes[0][0])\n",
    "        break\n",
    "    # if classes[0][0]==2:\n",
    "    #     print(theta_0_gpu)\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b76413d4-8837-41bb-860c-0d367333e85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "[CVAE_within_Gibbd] noise_gt: 0.03 | rmse 0.026313, noise_est: 0.025326(0.001090)\n"
     ]
    }
   ],
   "source": [
    "t = []\n",
    "for sigma_true in [0.03]: #\n",
    "    hypers['sigma_noise'] = sigma_true\n",
    "    sigma_noise = hypers['sigma_noise']\n",
    "    Hx = x\n",
    "    b = np.random.randn(n,50) * sigma_noise\n",
    "    y = Hx + b\n",
    "    # y = np.load(f'data/y_{sigma_noise}.npy')\n",
    "\n",
    "\n",
    "    THETA_mean = np.zeros([n,1])\n",
    "    THETA_var = np.zeros([n,1])\n",
    "    RMSE = np.zeros([n,1])\n",
    "\n",
    "\n",
    "    # np.random.seed(1)\n",
    "    # torch.manual_seed(14)\n",
    "    for j in range(n):    \n",
    "        # def Gibbs_sample(N,use_sample_size,x_truth,y,hypers,show_step=200,theta_truth=None):\n",
    "        N = 10000\n",
    "        use_sample_size = 5000\n",
    "        x_truth = x[j].reshape(-1,1) # 50,n\n",
    "        theta_truth = np.zeros(1)#??\n",
    "        m = hypers['x_dim']\n",
    "        y_j = y[j]\n",
    "\n",
    "        x_0 =  get_x_ml(y_j,hypers)\n",
    "        x_init_value = x_0\n",
    "#         init[j] = x_init_value\n",
    "\n",
    "        x_sum,x_square_sum = np.zeros_like(x_truth),np.zeros_like(x_truth)\n",
    "        theta_sum,theta_square_sum = np.zeros_like(theta_truth),np.zeros_like(theta_truth)  \n",
    "\n",
    "        z = torch.randn([1, args['latent_size']])\n",
    "        for i in range(N):\n",
    "            # sample theta\n",
    "            c = torch.cat((torch.tensor(x_0).float(),torch.tensor(y_j.reshape(-1,1)).float()),dim=0).T.to(device)\n",
    "            theta_0_gpu,_,classes = cvae.inference(z, torch.tensor(x_0.T).float(),torch.tensor(y_j.T).float().reshape(1,-1)) #更改\n",
    "            theta_0 = theta_0_gpu.detach().cpu().numpy()[0]\n",
    "          # 因为之前在训练网络的时候，训练数据的时候我们先标准化数据了。这个时候需要标准化回来。乘以的也是原本训练时候的均值与方差\n",
    "            # sample x\n",
    "            sigma = theta_0\n",
    "            t.append(sigma)\n",
    "\n",
    "            x_0,_,_ = x_post_sample_modify(m,hypers['sigma_prior'],sigma,y_j.reshape(-1,1))\n",
    "#             X.append(np.sqrt(((x_0 - x_truth)**2).sum()/50))\n",
    "\n",
    "            if i>(N-use_sample_size-1):\n",
    "                theta_sum,theta_square_sum = sample_cumulated_sum(theta_sum,theta_square_sum,theta_0)\n",
    "                x_sum,x_square_sum = sample_cumulated_sum(x_sum,x_square_sum,x_0)   \n",
    "\n",
    "        x_mean = x_sum/use_sample_size\n",
    "        x_var = x_square_sum/use_sample_size-x_mean**2\n",
    "\n",
    "        theta_mean = theta_sum/use_sample_size\n",
    "        theta_var = theta_square_sum/use_sample_size-theta_mean**2\n",
    "        rmse = np.sqrt(((x_mean - x_truth)**2).sum()/50)\n",
    "\n",
    "        THETA_mean[j] = theta_mean\n",
    "        THETA_var[j] = theta_var\n",
    "        RMSE[j] = rmse\n",
    "\n",
    "    print(classes[0][0])\n",
    "    noise_mean = THETA_mean.mean()\n",
    "    noise_std = np.sqrt(THETA_var).mean()\n",
    "    rmse_mean = RMSE.mean()\n",
    "    \n",
    "    print('[CVAE_within_Gibbd] noise_gt: %.2f | rmse %.6f, noise_est: %.6f(%.6f)' % (\n",
    "    sigma_true, rmse_mean, noise_mean,noise_std\n",
    "))\n",
    "# 每一个参数测试20次，每次固定参数以后，在第2次的时候，得到的x_mean将和对应的x_truth求RMSE。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c2909a86-63f4-4ebf-bb81-2365324cf4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/524_y_0.03',y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a90824-fe17-418f-8700-d9302d54d731",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "094fe7bd-f981-4692-940b-a61db46ca3fc",
   "metadata": {},
   "source": [
    "随机的。之前的比较刻意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3c80305e-c66a-4161-8e0e-f2eb947382ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "[CVAE_within_Gibbd] noise_gt: 0.01 | rmse 0.010018, noise_est: 0.025309(0.000716)\n",
      "tensor(0.)\n",
      "[CVAE_within_Gibbd] noise_gt: 0.03 | rmse 0.023998, noise_est: 0.024668(0.000451)\n",
      "tensor(1.)\n",
      "[CVAE_within_Gibbd] noise_gt: 0.05 | rmse 0.051287, noise_est: 0.031387(0.000503)\n"
     ]
    }
   ],
   "source": [
    "x_dim = hypers['x_dim']\n",
    "n = 1 # 一共有10个数据\n",
    "C = np.identity(x_dim) * (hypers['sigma_prior']**2)\n",
    "x = np.random.multivariate_normal([0] * x_dim, C,n) # [M,x_dim]\n",
    "t = []\n",
    "for sigma_true in [0.01,0.03,0.05]: #\n",
    "    hypers['sigma_noise'] = sigma_true\n",
    "    sigma_noise = hypers['sigma_noise']\n",
    "    Hx = x\n",
    "    b = np.random.randn(n,50) * sigma_noise\n",
    "    y = Hx + b\n",
    "    # y = np.load(f'data/y_{sigma_noise}.npy')\n",
    "\n",
    "\n",
    "    THETA_mean = np.zeros([n,1])\n",
    "    THETA_var = np.zeros([n,1])\n",
    "    RMSE = np.zeros([n,1])\n",
    "\n",
    "\n",
    "    # np.random.seed(1)\n",
    "    # torch.manual_seed(14)\n",
    "    for j in range(n):    \n",
    "        # def Gibbs_sample(N,use_sample_size,x_truth,y,hypers,show_step=200,theta_truth=None):\n",
    "        N = 10000\n",
    "        use_sample_size = 5000\n",
    "        x_truth = x[j].reshape(-1,1) # 50,n\n",
    "        theta_truth = np.zeros(1)#??\n",
    "        m = hypers['x_dim']\n",
    "        y_j = y[j]\n",
    "\n",
    "        x_0 =  get_x_ml(y_j,hypers)\n",
    "        x_init_value = x_0\n",
    "#         init[j] = x_init_value\n",
    "\n",
    "        x_sum,x_square_sum = np.zeros_like(x_truth),np.zeros_like(x_truth)\n",
    "        theta_sum,theta_square_sum = np.zeros_like(theta_truth),np.zeros_like(theta_truth)  \n",
    "\n",
    "        z = torch.randn([1, args['latent_size']])\n",
    "        for i in range(N):\n",
    "            # sample theta\n",
    "            c = torch.cat((torch.tensor(x_0).float(),torch.tensor(y_j.reshape(-1,1)).float()),dim=0).T.to(device)\n",
    "            theta_0_gpu,_,classes = cvae.inference(z, torch.tensor(x_0.T).float(),torch.tensor(y_j.T).float().reshape(1,-1)) #更改\n",
    "            theta_0 = theta_0_gpu.detach().cpu().numpy()[0]\n",
    "          # 因为之前在训练网络的时候，训练数据的时候我们先标准化数据了。这个时候需要标准化回来。乘以的也是原本训练时候的均值与方差\n",
    "            # sample x\n",
    "            sigma = theta_0\n",
    "            t.append(sigma)\n",
    "\n",
    "            x_0,_,_ = x_post_sample_modify(m,hypers['sigma_prior'],sigma,y_j.reshape(-1,1))\n",
    "#             X.append(np.sqrt(((x_0 - x_truth)**2).sum()/50))\n",
    "\n",
    "            if i>(N-use_sample_size-1):\n",
    "                theta_sum,theta_square_sum = sample_cumulated_sum(theta_sum,theta_square_sum,theta_0)\n",
    "                x_sum,x_square_sum = sample_cumulated_sum(x_sum,x_square_sum,x_0)   \n",
    "\n",
    "        x_mean = x_sum/use_sample_size\n",
    "        x_var = x_square_sum/use_sample_size-x_mean**2\n",
    "\n",
    "        theta_mean = theta_sum/use_sample_size\n",
    "        theta_var = theta_square_sum/use_sample_size-theta_mean**2\n",
    "        rmse = np.sqrt(((x_mean - x_truth)**2).sum()/50)\n",
    "\n",
    "        THETA_mean[j] = theta_mean\n",
    "        THETA_var[j] = theta_var\n",
    "        RMSE[j] = rmse\n",
    "\n",
    "    print(classes[0][0])\n",
    "    noise_mean = THETA_mean.mean()\n",
    "    noise_std = np.sqrt(THETA_var).mean()\n",
    "    rmse_mean = RMSE.mean()\n",
    "    \n",
    "    print('[CVAE_within_Gibbd] noise_gt: %.2f | rmse %.6f, noise_est: %.6f(%.6f)' % (\n",
    "    sigma_true, rmse_mean, noise_mean,noise_std\n",
    "))\n",
    "\n",
    "    \n",
    "\n",
    "# 每一个参数测试20次，每次固定参数以后，在第2次的时候，得到的x_mean将和对应的x_truth求RMSE。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd39f51e-7034-4997-b738-7fa8357779a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2340159-00c8-40cf-8f76-0a421c13f198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2610c507400>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x_mean,x_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f82ec5-091e-4f6c-9753-70af5384d155",
   "metadata": {},
   "source": [
    "# AGEM signal denoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "787dafb3-70d5-4800-9979-0ef79a6f6f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "82c1eedd-8b5d-49ad-bd32-b9eb2875ef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load('data/524_x.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "95eca8cd-6271-4786-bcae-80189f1b45b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01]\n",
      "epoch 0, time 0.1 sec | noise_gt: 0.0100, noise_est: 0.0100 (0.0003) | rmse: 0.0082 (0.0055)\n",
      "epoch 1, time 0.1 sec | noise_gt: 0.0100, noise_est: 0.0101 (0.0005) | rmse: 0.0083 (0.0056)\n",
      "epoch 2, time 0.0 sec | noise_gt: 0.0100, noise_est: 0.0101 (0.0006) | rmse: 0.0085 (0.0055)\n",
      "epoch 3, time 0.1 sec | noise_gt: 0.0100, noise_est: 0.0101 (0.0008) | rmse: 0.0084 (0.0055)\n",
      "epoch 4, time 0.1 sec | noise_gt: 0.0100, noise_est: 0.0101 (0.0008) | rmse: 0.0083 (0.0055)\n",
      "epoch 5, time 0.0 sec | noise_gt: 0.0100, noise_est: 0.0102 (0.0009) | rmse: 0.0084 (0.0056)\n",
      "epoch 6, time 0.0 sec | noise_gt: 0.0100, noise_est: 0.0103 (0.0011) | rmse: 0.0084 (0.0057)\n",
      "epoch 7, time 0.1 sec | noise_gt: 0.0100, noise_est: 0.0104 (0.0012) | rmse: 0.0084 (0.0057)\n",
      "epoch 8, time 0.1 sec | noise_gt: 0.0100, noise_est: 0.0104 (0.0014) | rmse: 0.0084 (0.0057)\n",
      "epoch 9, time 0.1 sec | noise_gt: 0.0100, noise_est: 0.0105 (0.0017) | rmse: 0.0084 (0.0057)\n",
      "[AGEM] noise_gt: 0.01 | rmse 0.0084 (0.0057), noise_est: 0.0105 (0.0017)\n",
      "[0.03]\n",
      "epoch 0, time 0.0 sec | noise_gt: 0.0300, noise_est: 0.0101 (0.0004) | rmse: 0.0237 (0.0159)\n",
      "epoch 1, time 0.1 sec | noise_gt: 0.0300, noise_est: 0.0103 (0.0006) | rmse: 0.0238 (0.0158)\n",
      "epoch 2, time 0.0 sec | noise_gt: 0.0300, noise_est: 0.0104 (0.0007) | rmse: 0.0237 (0.0160)\n",
      "epoch 3, time 0.1 sec | noise_gt: 0.0300, noise_est: 0.0104 (0.0008) | rmse: 0.0235 (0.0159)\n",
      "epoch 4, time 0.1 sec | noise_gt: 0.0300, noise_est: 0.0105 (0.0010) | rmse: 0.0237 (0.0159)\n",
      "epoch 5, time 0.1 sec | noise_gt: 0.0300, noise_est: 0.0106 (0.0011) | rmse: 0.0238 (0.0158)\n",
      "epoch 6, time 0.1 sec | noise_gt: 0.0300, noise_est: 0.0107 (0.0012) | rmse: 0.0238 (0.0159)\n",
      "epoch 7, time 0.1 sec | noise_gt: 0.0300, noise_est: 0.0109 (0.0013) | rmse: 0.0237 (0.0157)\n",
      "epoch 8, time 0.0 sec | noise_gt: 0.0300, noise_est: 0.0110 (0.0014) | rmse: 0.0239 (0.0161)\n",
      "epoch 9, time 0.1 sec | noise_gt: 0.0300, noise_est: 0.0111 (0.0016) | rmse: 0.0239 (0.0161)\n",
      "[AGEM] noise_gt: 0.03 | rmse 0.0239 (0.0161), noise_est: 0.0111 (0.0016)\n",
      "[0.05]\n",
      "epoch 0, time 0.1 sec | noise_gt: 0.0500, noise_est: 0.0101 (0.0004) | rmse: 0.0374 (0.0276)\n",
      "epoch 1, time 0.0 sec | noise_gt: 0.0500, noise_est: 0.0102 (0.0006) | rmse: 0.0375 (0.0278)\n",
      "epoch 2, time 0.1 sec | noise_gt: 0.0500, noise_est: 0.0104 (0.0009) | rmse: 0.0378 (0.0279)\n",
      "epoch 3, time 0.0 sec | noise_gt: 0.0500, noise_est: 0.0105 (0.0011) | rmse: 0.0375 (0.0277)\n",
      "epoch 4, time 0.1 sec | noise_gt: 0.0500, noise_est: 0.0106 (0.0013) | rmse: 0.0378 (0.0278)\n",
      "epoch 5, time 0.1 sec | noise_gt: 0.0500, noise_est: 0.0108 (0.0015) | rmse: 0.0378 (0.0278)\n",
      "epoch 6, time 0.1 sec | noise_gt: 0.0500, noise_est: 0.0109 (0.0016) | rmse: 0.0377 (0.0282)\n",
      "epoch 7, time 0.0 sec | noise_gt: 0.0500, noise_est: 0.0111 (0.0019) | rmse: 0.0380 (0.0281)\n",
      "epoch 8, time 0.1 sec | noise_gt: 0.0500, noise_est: 0.0112 (0.0023) | rmse: 0.0378 (0.0280)\n",
      "epoch 9, time 0.1 sec | noise_gt: 0.0500, noise_est: 0.0114 (0.0033) | rmse: 0.0379 (0.0283)\n",
      "[AGEM] noise_gt: 0.05 | rmse 0.0379 (0.0283), noise_est: 0.0114 (0.0033)\n"
     ]
    }
   ],
   "source": [
    "from AGEM.solver import solve_agem\n",
    "import numpy as np\n",
    "\n",
    "AGEM_samples = []\n",
    "def run_test():\n",
    "    x_dim = 50\n",
    "    sigma_prior = hypers['sigma_prior'] # 对应新方法的 sigma_prior\n",
    "    sigma_proposal = 0.01\n",
    "    sigma_noise_hat_init = 0.01 # 初始项\n",
    "    cov = np.identity(x_dim) * sigma_prior**2\n",
    "    x_test = x\n",
    "    x_true = x_test[:]\n",
    "\n",
    "    noise_shape = x_true.shape[1:]\n",
    "    n_dim = np.prod(noise_shape)\n",
    "\n",
    "\n",
    "    for noise in [0.01,0.03,0.05]: #??   # 对应sigma_noise ，为真实的噪音取值！\n",
    "        sigma_noise = [noise] * n_dim\n",
    "        sigma_noise = np.array(sigma_noise[:n_dim]).reshape(noise_shape)\n",
    "        print(sigma_noise)\n",
    "        rmse_mean, rmse_std, noise_mean, noise_std,estimation = \\\n",
    "            solve_agem(x_true=x_true, sigma_noise=sigma_noise,\n",
    "                       sigma_prior=sigma_prior, sigma_noise_hat_init=sigma_noise_hat_init,\n",
    "                       sigma_proposal=sigma_proposal, type_proposal='mala',\n",
    "                       candidate='mean', em_epochs=10, sample_epochs=1000)\n",
    "\n",
    "        print('[AGEM] noise_gt: %.2f | rmse %.4f (%.4f), noise_est: %.4f (%.4f)' % (\n",
    "            noise, rmse_mean, rmse_std, noise_mean, noise_std\n",
    "        ))\n",
    "        AGEM_samples.append(estimation)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e0b28d05-c8c1-477d-a91b-c096c870ce0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 20, 50)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AGEM_samples = np.array(AGEM_samples)\n",
    "AGEM_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "54e1694c-0d7c-49f2-9c92-0c5ed0dceb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGEM = pd.DataFrame(np.vstack((AGEM_samples[0][2].flatten(),AGEM_samples[1][2].flatten(),AGEM_samples[2][2].flatten())))\n",
    "AGEM.to_csv('AGEM_samples.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "66303eab-3ea3-4d69-aa5f-cf23395db881",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGEM_samples_mean = np.mean(AGEM_samples_mean,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e518bfc4-2a1f-410f-a274-3fd5ba544426",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGEM_samples_mean = pd.DataFrame(AGEM_samples_mean)\n",
    "AGEM_samples_mean.to_csv('AGEM_samples_mean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0650f330-9d0b-4664-ae91-9d2a29e186cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = pd.DataFrame(P.reshape(4,50))\n",
    "P.to_csv('our_mean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e051db8a-3275-40d8-bfac-c4265cdc70c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000509</td>\n",
       "      <td>-0.000121</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>-0.000174</td>\n",
       "      <td>-0.000148</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.001298</td>\n",
       "      <td>-0.000202</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>-0.000132</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000447</td>\n",
       "      <td>-0.000074</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>-0.000113</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>-0.000118</td>\n",
       "      <td>-0.000571</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>-0.000582</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>-0.000104</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>-0.000495</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>-0.000345</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.000459</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>-0.000657</td>\n",
       "      <td>-0.000495</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>-0.000347</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>-0.000167</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>-0.000207</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000296</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>-0.000164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001068</td>\n",
       "      <td>-0.000716</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>-0.000306</td>\n",
       "      <td>0.001081</td>\n",
       "      <td>-0.000398</td>\n",
       "      <td>-0.002848</td>\n",
       "      <td>-0.000354</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>-0.000581</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>-0.000911</td>\n",
       "      <td>-0.000476</td>\n",
       "      <td>0.000942</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>0.000725</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000683</td>\n",
       "      <td>-0.000592</td>\n",
       "      <td>-0.001419</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>-0.001005</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>-0.000188</td>\n",
       "      <td>-0.000124</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>-0.000458</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>-0.000801</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>-0.000706</td>\n",
       "      <td>-0.000645</td>\n",
       "      <td>-0.000805</td>\n",
       "      <td>-0.001356</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>-0.000369</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>-0.000973</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>-0.000151</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.001426</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>-0.000509</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>-0.000218</td>\n",
       "      <td>-0.003771</td>\n",
       "      <td>-0.000591</td>\n",
       "      <td>-0.000198</td>\n",
       "      <td>-0.001444</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>-0.001261</td>\n",
       "      <td>-0.000597</td>\n",
       "      <td>0.001267</td>\n",
       "      <td>-0.000341</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>0.000778</td>\n",
       "      <td>-0.000653</td>\n",
       "      <td>-0.001775</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.001305</td>\n",
       "      <td>-0.000176</td>\n",
       "      <td>-0.000579</td>\n",
       "      <td>-0.000192</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>-0.000695</td>\n",
       "      <td>0.001938</td>\n",
       "      <td>-0.001122</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>-0.000565</td>\n",
       "      <td>-0.001405</td>\n",
       "      <td>-0.000770</td>\n",
       "      <td>-0.001530</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>-0.000308</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>0.001423</td>\n",
       "      <td>-0.001568</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000623</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>-0.000285</td>\n",
       "      <td>0.000395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.001966</td>\n",
       "      <td>-0.001163</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>-0.000364</td>\n",
       "      <td>0.002165</td>\n",
       "      <td>-0.000489</td>\n",
       "      <td>-0.004993</td>\n",
       "      <td>-0.000813</td>\n",
       "      <td>-0.000379</td>\n",
       "      <td>-0.002093</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>-0.001706</td>\n",
       "      <td>-0.001091</td>\n",
       "      <td>0.001909</td>\n",
       "      <td>-0.000720</td>\n",
       "      <td>0.000986</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>-0.001199</td>\n",
       "      <td>-0.002514</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>-0.001795</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>-0.000683</td>\n",
       "      <td>-0.000129</td>\n",
       "      <td>-0.000144</td>\n",
       "      <td>-0.000632</td>\n",
       "      <td>0.002754</td>\n",
       "      <td>-0.001543</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>-0.000859</td>\n",
       "      <td>-0.001893</td>\n",
       "      <td>-0.000655</td>\n",
       "      <td>-0.002065</td>\n",
       "      <td>0.001277</td>\n",
       "      <td>-0.000277</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.002324</td>\n",
       "      <td>-0.002218</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>0.000912</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.000642</td>\n",
       "      <td>-0.000704</td>\n",
       "      <td>0.000706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15        16        17        18        19        20        21        22        23        24        25        26        27        28        29        30        31        32        33        34        35        36        37        38        39        40        41        42        43        44        45        46        47        48        49\n",
       "0 -0.000509 -0.000121  0.000108 -0.000174 -0.000148  0.000198 -0.000040 -0.001298 -0.000202  0.000308 -0.000132  0.000108 -0.000036 -0.000447 -0.000074  0.000336  0.000027  0.000419 -0.000113  0.000373 -0.000118 -0.000571  0.000176 -0.000582  0.000001  0.000236 -0.000104  0.000138 -0.000495  0.000434 -0.000345  0.000029 -0.000459  0.000101 -0.000657 -0.000495  0.000031 -0.000347  0.000960  0.000201 -0.000167  0.000118  0.000204  0.000147 -0.000207  0.000242 -0.000026 -0.000296  0.000486 -0.000164\n",
       "1 -0.001068 -0.000716  0.000503  0.000105 -0.000306  0.001081 -0.000398 -0.002848 -0.000354  0.000163 -0.000581  0.000487  0.000039 -0.000911 -0.000476  0.000942 -0.000160  0.000725  0.000118  0.000683 -0.000592 -0.001419  0.000035 -0.001005  0.000078 -0.000188 -0.000124  0.000158 -0.000458  0.001303 -0.000801  0.000096 -0.000706 -0.000645 -0.000805 -0.001356  0.000466 -0.000369  0.000920  0.000727 -0.000973  0.000100  0.000526  0.000423  0.000089  0.000538  0.000061 -0.000151  0.000056  0.000039\n",
       "2 -0.001426 -0.000945  0.000539  0.000308 -0.000509  0.001520 -0.000218 -0.003771 -0.000591 -0.000198 -0.001444  0.000816  0.000138 -0.001261 -0.000597  0.001267 -0.000341  0.000913  0.000706  0.000778 -0.000653 -0.001775 -0.000062 -0.001305 -0.000176 -0.000579 -0.000192  0.000160 -0.000695  0.001938 -0.001122  0.000059 -0.000565 -0.001405 -0.000770 -0.001530  0.000622 -0.000308  0.000826  0.001423 -0.001568  0.000142  0.000788  0.000581  0.000130  0.000623  0.000328  0.000087 -0.000285  0.000395\n",
       "3 -0.001966 -0.001163  0.000925  0.000331 -0.000364  0.002165 -0.000489 -0.004993 -0.000813 -0.000379 -0.002093  0.000724  0.000038 -0.001706 -0.001091  0.001909 -0.000720  0.000986  0.000332  0.000997 -0.001199 -0.002514  0.000128 -0.001795  0.000183 -0.000683 -0.000129 -0.000144 -0.000632  0.002754 -0.001543  0.000441 -0.000859 -0.001893 -0.000655 -0.002065  0.001277 -0.000277  0.000789  0.002324 -0.002218  0.000164  0.001142  0.000794  0.000719  0.000912  0.000322  0.000642 -0.000704  0.000706"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ed532379-2ead-44ca-ab01-7faa84df3209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.000509</td>\n",
       "      <td>-0.000121</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>-0.000174</td>\n",
       "      <td>-0.000148</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.001298</td>\n",
       "      <td>-0.000202</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>-0.000132</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000447</td>\n",
       "      <td>-0.000074</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>-0.000113</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>-0.000118</td>\n",
       "      <td>-0.000571</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>-0.000582</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>-0.000104</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>-0.000495</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>-0.000345</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.000459</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>-0.000657</td>\n",
       "      <td>-0.000495</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>-0.000347</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>-0.000167</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>-0.000207</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000296</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>-0.000164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.001068</td>\n",
       "      <td>-0.000716</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>-0.000306</td>\n",
       "      <td>0.001081</td>\n",
       "      <td>-0.000398</td>\n",
       "      <td>-0.002848</td>\n",
       "      <td>-0.000354</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>-0.000581</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>-0.000911</td>\n",
       "      <td>-0.000476</td>\n",
       "      <td>0.000942</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>0.000725</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000683</td>\n",
       "      <td>-0.000592</td>\n",
       "      <td>-0.001419</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>-0.001005</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>-0.000188</td>\n",
       "      <td>-0.000124</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>-0.000458</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>-0.000801</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>-0.000706</td>\n",
       "      <td>-0.000645</td>\n",
       "      <td>-0.000805</td>\n",
       "      <td>-0.001356</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>-0.000369</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>-0.000973</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>-0.000151</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.001426</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>-0.000509</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>-0.000218</td>\n",
       "      <td>-0.003771</td>\n",
       "      <td>-0.000591</td>\n",
       "      <td>-0.000198</td>\n",
       "      <td>-0.001444</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>-0.001261</td>\n",
       "      <td>-0.000597</td>\n",
       "      <td>0.001267</td>\n",
       "      <td>-0.000341</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>0.000778</td>\n",
       "      <td>-0.000653</td>\n",
       "      <td>-0.001775</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.001305</td>\n",
       "      <td>-0.000176</td>\n",
       "      <td>-0.000579</td>\n",
       "      <td>-0.000192</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>-0.000695</td>\n",
       "      <td>0.001938</td>\n",
       "      <td>-0.001122</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>-0.000565</td>\n",
       "      <td>-0.001405</td>\n",
       "      <td>-0.000770</td>\n",
       "      <td>-0.001530</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>-0.000308</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>0.001423</td>\n",
       "      <td>-0.001568</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000623</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>-0.000285</td>\n",
       "      <td>0.000395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.001966</td>\n",
       "      <td>-0.001163</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>-0.000364</td>\n",
       "      <td>0.002165</td>\n",
       "      <td>-0.000489</td>\n",
       "      <td>-0.004993</td>\n",
       "      <td>-0.000813</td>\n",
       "      <td>-0.000379</td>\n",
       "      <td>-0.002093</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>-0.001706</td>\n",
       "      <td>-0.001091</td>\n",
       "      <td>0.001909</td>\n",
       "      <td>-0.000720</td>\n",
       "      <td>0.000986</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>-0.001199</td>\n",
       "      <td>-0.002514</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>-0.001795</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>-0.000683</td>\n",
       "      <td>-0.000129</td>\n",
       "      <td>-0.000144</td>\n",
       "      <td>-0.000632</td>\n",
       "      <td>0.002754</td>\n",
       "      <td>-0.001543</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>-0.000859</td>\n",
       "      <td>-0.001893</td>\n",
       "      <td>-0.000655</td>\n",
       "      <td>-0.002065</td>\n",
       "      <td>0.001277</td>\n",
       "      <td>-0.000277</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.002324</td>\n",
       "      <td>-0.002218</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>0.000912</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.000642</td>\n",
       "      <td>-0.000704</td>\n",
       "      <td>0.000706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         0         1         2         3         4         5         6         7         8         9        10        11        12        13        14        15        16        17        18        19        20        21        22        23        24        25        26        27        28        29        30        31        32        33        34        35        36        37        38        39        40        41        42        43        44        45        46        47        48        49\n",
       "0           0 -0.000509 -0.000121  0.000108 -0.000174 -0.000148  0.000198 -0.000040 -0.001298 -0.000202  0.000308 -0.000132  0.000108 -0.000036 -0.000447 -0.000074  0.000336  0.000027  0.000419 -0.000113  0.000373 -0.000118 -0.000571  0.000176 -0.000582  0.000001  0.000236 -0.000104  0.000138 -0.000495  0.000434 -0.000345  0.000029 -0.000459  0.000101 -0.000657 -0.000495  0.000031 -0.000347  0.000960  0.000201 -0.000167  0.000118  0.000204  0.000147 -0.000207  0.000242 -0.000026 -0.000296  0.000486 -0.000164\n",
       "1           1 -0.001068 -0.000716  0.000503  0.000105 -0.000306  0.001081 -0.000398 -0.002848 -0.000354  0.000163 -0.000581  0.000487  0.000039 -0.000911 -0.000476  0.000942 -0.000160  0.000725  0.000118  0.000683 -0.000592 -0.001419  0.000035 -0.001005  0.000078 -0.000188 -0.000124  0.000158 -0.000458  0.001303 -0.000801  0.000096 -0.000706 -0.000645 -0.000805 -0.001356  0.000466 -0.000369  0.000920  0.000727 -0.000973  0.000100  0.000526  0.000423  0.000089  0.000538  0.000061 -0.000151  0.000056  0.000039\n",
       "2           2 -0.001426 -0.000945  0.000539  0.000308 -0.000509  0.001520 -0.000218 -0.003771 -0.000591 -0.000198 -0.001444  0.000816  0.000138 -0.001261 -0.000597  0.001267 -0.000341  0.000913  0.000706  0.000778 -0.000653 -0.001775 -0.000062 -0.001305 -0.000176 -0.000579 -0.000192  0.000160 -0.000695  0.001938 -0.001122  0.000059 -0.000565 -0.001405 -0.000770 -0.001530  0.000622 -0.000308  0.000826  0.001423 -0.001568  0.000142  0.000788  0.000581  0.000130  0.000623  0.000328  0.000087 -0.000285  0.000395\n",
       "3           3 -0.001966 -0.001163  0.000925  0.000331 -0.000364  0.002165 -0.000489 -0.004993 -0.000813 -0.000379 -0.002093  0.000724  0.000038 -0.001706 -0.001091  0.001909 -0.000720  0.000986  0.000332  0.000997 -0.001199 -0.002514  0.000128 -0.001795  0.000183 -0.000683 -0.000129 -0.000144 -0.000632  0.002754 -0.001543  0.000441 -0.000859 -0.001893 -0.000655 -0.002065  0.001277 -0.000277  0.000789  0.002324 -0.002218  0.000164  0.001142  0.000794  0.000719  0.000912  0.000322  0.000642 -0.000704  0.000706"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('.\\our_mean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "58799322-a53e-4e0d-94dc-f8ce6bb0eac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 50)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = pd.read_csv('.\\our_mean.csv')\n",
    "data1 = data1.drop(data1.columns[[0]],axis = 1)\n",
    "data1 = np.array(data1)\n",
    "data1.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "acf668eb-f30c-44d0-a919-afe4454b085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('AGEM_samples_mean.csv')\n",
    "data2 = data2.drop(data2.columns[[0]],axis = 1)\n",
    "data2 = np.array(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0da49a4-9b34-4b61-b939-1f75ac6aa732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sigma0.01</th>\n",
       "      <th>sigma0.03</th>\n",
       "      <th>sigma0.05</th>\n",
       "      <th>kind</th>\n",
       "      <th>TRUE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.020061</td>\n",
       "      <td>-0.012072</td>\n",
       "      <td>-0.006576</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>-0.009517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002311</td>\n",
       "      <td>0.020589</td>\n",
       "      <td>0.060092</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>0.004325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.007509</td>\n",
       "      <td>0.019467</td>\n",
       "      <td>0.016773</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>0.008074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.005890</td>\n",
       "      <td>-0.011799</td>\n",
       "      <td>-0.005170</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>-0.014607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.020143</td>\n",
       "      <td>-0.027948</td>\n",
       "      <td>-0.033163</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>0.005725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.003074</td>\n",
       "      <td>0.015191</td>\n",
       "      <td>0.006413</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>-0.001085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.006570</td>\n",
       "      <td>0.028322</td>\n",
       "      <td>0.020603</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>0.007790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.000660</td>\n",
       "      <td>-0.021008</td>\n",
       "      <td>-0.026160</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>0.001461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.022022</td>\n",
       "      <td>-0.014411</td>\n",
       "      <td>-0.033571</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>0.005319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.005865</td>\n",
       "      <td>-0.004974</td>\n",
       "      <td>-0.013814</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>-0.001234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.020719</td>\n",
       "      <td>-0.033598</td>\n",
       "      <td>-0.043010</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>-0.004888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.007203</td>\n",
       "      <td>-0.021965</td>\n",
       "      <td>-0.000230</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>-0.007508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.001269</td>\n",
       "      <td>0.011078</td>\n",
       "      <td>0.020061</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>-0.012511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.020687</td>\n",
       "      <td>-0.017162</td>\n",
       "      <td>-0.023987</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>0.005134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.005479</td>\n",
       "      <td>0.008202</td>\n",
       "      <td>-0.002340</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>-0.002911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.015106</td>\n",
       "      <td>-0.025426</td>\n",
       "      <td>-0.019245</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>0.020807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.013663</td>\n",
       "      <td>-0.020239</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>-0.002008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.032937</td>\n",
       "      <td>-0.054281</td>\n",
       "      <td>-0.063944</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>-0.014186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.006025</td>\n",
       "      <td>0.035082</td>\n",
       "      <td>0.027306</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>-0.001978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.007789</td>\n",
       "      <td>0.008427</td>\n",
       "      <td>0.006995</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>0.004155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.022752</td>\n",
       "      <td>-0.004582</td>\n",
       "      <td>-0.007688</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>0.006029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.007569</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.026595</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>0.018531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.009349</td>\n",
       "      <td>0.017062</td>\n",
       "      <td>0.032196</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>0.001572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.025619</td>\n",
       "      <td>0.059972</td>\n",
       "      <td>0.051372</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>0.007357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.015989</td>\n",
       "      <td>-0.017729</td>\n",
       "      <td>-0.056959</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>0.015958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.014415</td>\n",
       "      <td>0.021083</td>\n",
       "      <td>0.035746</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>-0.001029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.005668</td>\n",
       "      <td>0.015993</td>\n",
       "      <td>0.044969</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>-0.011276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.008578</td>\n",
       "      <td>0.013374</td>\n",
       "      <td>0.007327</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>0.000555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.009356</td>\n",
       "      <td>-0.013169</td>\n",
       "      <td>-0.046282</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>-0.002350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.023946</td>\n",
       "      <td>0.004695</td>\n",
       "      <td>0.014547</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>-0.016656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-0.026142</td>\n",
       "      <td>-0.008340</td>\n",
       "      <td>-0.007104</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>-0.006430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-0.008547</td>\n",
       "      <td>-0.008139</td>\n",
       "      <td>-0.026314</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>0.009348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-0.042110</td>\n",
       "      <td>-0.020934</td>\n",
       "      <td>-0.037529</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>-0.006956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.034739</td>\n",
       "      <td>0.014169</td>\n",
       "      <td>0.009556</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>0.000473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.004372</td>\n",
       "      <td>0.012912</td>\n",
       "      <td>0.020812</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>-0.009057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-0.007951</td>\n",
       "      <td>-0.011594</td>\n",
       "      <td>-0.029266</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>-0.003753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.000325</td>\n",
       "      <td>0.003670</td>\n",
       "      <td>0.012586</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>0.003291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.016364</td>\n",
       "      <td>-0.027040</td>\n",
       "      <td>-0.034595</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>-0.000974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.014332</td>\n",
       "      <td>0.028319</td>\n",
       "      <td>0.038419</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>0.010410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-0.002607</td>\n",
       "      <td>0.009706</td>\n",
       "      <td>0.005041</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>-0.001042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.030848</td>\n",
       "      <td>0.004813</td>\n",
       "      <td>0.003535</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>0.015966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.006986</td>\n",
       "      <td>-0.000695</td>\n",
       "      <td>-0.005842</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>-0.001986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.023833</td>\n",
       "      <td>0.020406</td>\n",
       "      <td>0.015013</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>-0.016377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.000883</td>\n",
       "      <td>0.025850</td>\n",
       "      <td>0.018390</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>0.001838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.031621</td>\n",
       "      <td>0.030056</td>\n",
       "      <td>0.023568</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>0.013625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-0.002505</td>\n",
       "      <td>-0.003297</td>\n",
       "      <td>0.001657</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>0.004434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.010093</td>\n",
       "      <td>-0.010653</td>\n",
       "      <td>0.004044</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>0.004172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.015977</td>\n",
       "      <td>0.018654</td>\n",
       "      <td>0.022958</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>0.003526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>-0.003602</td>\n",
       "      <td>0.011838</td>\n",
       "      <td>0.009226</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>-0.006044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.010555</td>\n",
       "      <td>0.002937</td>\n",
       "      <td>0.026020</td>\n",
       "      <td>AGEM</td>\n",
       "      <td>-0.004604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.000999</td>\n",
       "      <td>0.004510</td>\n",
       "      <td>0.005656</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>-0.009517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>-0.000529</td>\n",
       "      <td>-0.002978</td>\n",
       "      <td>-0.003715</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>0.004325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>-0.000115</td>\n",
       "      <td>-0.002517</td>\n",
       "      <td>-0.003335</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>0.008074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>-0.000236</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.002693</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>-0.014607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>-0.000452</td>\n",
       "      <td>-0.002632</td>\n",
       "      <td>-0.003337</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>0.005725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.002497</td>\n",
       "      <td>0.005925</td>\n",
       "      <td>0.006849</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>-0.001085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.000011</td>\n",
       "      <td>-0.000897</td>\n",
       "      <td>-0.001330</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>0.007790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>-0.000488</td>\n",
       "      <td>-0.001826</td>\n",
       "      <td>-0.002207</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>0.001461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.003360</td>\n",
       "      <td>0.006606</td>\n",
       "      <td>0.007327</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>0.005319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.002615</td>\n",
       "      <td>0.005246</td>\n",
       "      <td>0.005975</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>-0.001234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.001486</td>\n",
       "      <td>0.004863</td>\n",
       "      <td>0.005861</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>-0.004888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.002311</td>\n",
       "      <td>0.002959</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>-0.007508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>-0.001790</td>\n",
       "      <td>-0.002999</td>\n",
       "      <td>-0.002998</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>-0.012511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.003368</td>\n",
       "      <td>0.006597</td>\n",
       "      <td>0.007319</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>0.005134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>-0.001063</td>\n",
       "      <td>-0.001470</td>\n",
       "      <td>-0.001510</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>-0.002911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.003826</td>\n",
       "      <td>0.004755</td>\n",
       "      <td>0.004493</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>0.020807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.001139</td>\n",
       "      <td>0.002845</td>\n",
       "      <td>0.003343</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>-0.002008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>-0.001230</td>\n",
       "      <td>-0.001166</td>\n",
       "      <td>-0.000793</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>-0.014186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.000458</td>\n",
       "      <td>0.001577</td>\n",
       "      <td>0.001916</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>-0.001978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.001727</td>\n",
       "      <td>0.003247</td>\n",
       "      <td>0.003542</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>0.004155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.001793</td>\n",
       "      <td>0.002321</td>\n",
       "      <td>0.002320</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>0.006029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.001183</td>\n",
       "      <td>-0.001162</td>\n",
       "      <td>-0.002223</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>0.018531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.000786</td>\n",
       "      <td>0.001275</td>\n",
       "      <td>0.001367</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>0.001572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.002322</td>\n",
       "      <td>0.004470</td>\n",
       "      <td>0.004848</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>0.007357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.001005</td>\n",
       "      <td>-0.000578</td>\n",
       "      <td>-0.001383</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>0.015958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>-0.000121</td>\n",
       "      <td>-0.000357</td>\n",
       "      <td>-0.000391</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>-0.001029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>-0.001252</td>\n",
       "      <td>-0.001746</td>\n",
       "      <td>-0.001590</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>-0.011276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.002994</td>\n",
       "      <td>0.006225</td>\n",
       "      <td>0.007063</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>0.000555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.001491</td>\n",
       "      <td>0.001864</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>-0.002350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>-0.001551</td>\n",
       "      <td>-0.000456</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>-0.016656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>-0.002028</td>\n",
       "      <td>-0.003231</td>\n",
       "      <td>-0.003392</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>-0.006430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.002507</td>\n",
       "      <td>0.002468</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>0.009348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>-0.001806</td>\n",
       "      <td>-0.002828</td>\n",
       "      <td>-0.002926</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>-0.006956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>-0.000677</td>\n",
       "      <td>-0.000642</td>\n",
       "      <td>-0.000655</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>0.000473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.002787</td>\n",
       "      <td>-0.002801</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>-0.009057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>0.001958</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>-0.003753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.001908</td>\n",
       "      <td>0.002069</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>0.003291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000374</td>\n",
       "      <td>-0.000434</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>-0.000974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.002067</td>\n",
       "      <td>0.002524</td>\n",
       "      <td>0.002393</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>0.010410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>-0.001811</td>\n",
       "      <td>-0.003704</td>\n",
       "      <td>-0.004177</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>-0.001042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.000513</td>\n",
       "      <td>-0.002457</td>\n",
       "      <td>-0.003617</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>0.015966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.001264</td>\n",
       "      <td>0.001527</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>-0.001986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>-0.001761</td>\n",
       "      <td>-0.001092</td>\n",
       "      <td>-0.000517</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>-0.016377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.001912</td>\n",
       "      <td>0.002143</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>0.001838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.005048</td>\n",
       "      <td>0.008910</td>\n",
       "      <td>0.009590</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>0.013625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.001919</td>\n",
       "      <td>0.003473</td>\n",
       "      <td>0.003772</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>0.004434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-0.000501</td>\n",
       "      <td>-0.002192</td>\n",
       "      <td>-0.002734</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>0.004172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.001710</td>\n",
       "      <td>0.003582</td>\n",
       "      <td>0.003983</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>0.003526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-0.001981</td>\n",
       "      <td>-0.004139</td>\n",
       "      <td>-0.004549</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>-0.006044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-0.000434</td>\n",
       "      <td>-0.001126</td>\n",
       "      <td>-0.001185</td>\n",
       "      <td>CVAE-within-Gibbs</td>\n",
       "      <td>-0.004604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sigma0.01  sigma0.03  sigma0.05               kind      TRUE\n",
       "0   -0.020061  -0.012072  -0.006576               AGEM -0.009517\n",
       "1    0.002311   0.020589   0.060092               AGEM  0.004325\n",
       "2    0.007509   0.019467   0.016773               AGEM  0.008074\n",
       "3   -0.005890  -0.011799  -0.005170               AGEM -0.014607\n",
       "4   -0.020143  -0.027948  -0.033163               AGEM  0.005725\n",
       "5    0.003074   0.015191   0.006413               AGEM -0.001085\n",
       "6   -0.006570   0.028322   0.020603               AGEM  0.007790\n",
       "7   -0.000660  -0.021008  -0.026160               AGEM  0.001461\n",
       "8    0.022022  -0.014411  -0.033571               AGEM  0.005319\n",
       "9   -0.005865  -0.004974  -0.013814               AGEM -0.001234\n",
       "10  -0.020719  -0.033598  -0.043010               AGEM -0.004888\n",
       "11  -0.007203  -0.021965  -0.000230               AGEM -0.007508\n",
       "12   0.001269   0.011078   0.020061               AGEM -0.012511\n",
       "13  -0.020687  -0.017162  -0.023987               AGEM  0.005134\n",
       "14   0.005479   0.008202  -0.002340               AGEM -0.002911\n",
       "15  -0.015106  -0.025426  -0.019245               AGEM  0.020807\n",
       "16   0.013663  -0.020239   0.000702               AGEM -0.002008\n",
       "17  -0.032937  -0.054281  -0.063944               AGEM -0.014186\n",
       "18  -0.006025   0.035082   0.027306               AGEM -0.001978\n",
       "19   0.007789   0.008427   0.006995               AGEM  0.004155\n",
       "20  -0.022752  -0.004582  -0.007688               AGEM  0.006029\n",
       "21  -0.007569  -0.000006  -0.026595               AGEM  0.018531\n",
       "22   0.009349   0.017062   0.032196               AGEM  0.001572\n",
       "23   0.025619   0.059972   0.051372               AGEM  0.007357\n",
       "24   0.015989  -0.017729  -0.056959               AGEM  0.015958\n",
       "25  -0.014415   0.021083   0.035746               AGEM -0.001029\n",
       "26   0.005668   0.015993   0.044969               AGEM -0.011276\n",
       "27   0.008578   0.013374   0.007327               AGEM  0.000555\n",
       "28   0.009356  -0.013169  -0.046282               AGEM -0.002350\n",
       "29  -0.023946   0.004695   0.014547               AGEM -0.016656\n",
       "30  -0.026142  -0.008340  -0.007104               AGEM -0.006430\n",
       "31  -0.008547  -0.008139  -0.026314               AGEM  0.009348\n",
       "32  -0.042110  -0.020934  -0.037529               AGEM -0.006956\n",
       "33   0.034739   0.014169   0.009556               AGEM  0.000473\n",
       "34  -0.004372   0.012912   0.020812               AGEM -0.009057\n",
       "35  -0.007951  -0.011594  -0.029266               AGEM -0.003753\n",
       "36  -0.000325   0.003670   0.012586               AGEM  0.003291\n",
       "37   0.016364  -0.027040  -0.034595               AGEM -0.000974\n",
       "38   0.014332   0.028319   0.038419               AGEM  0.010410\n",
       "39  -0.002607   0.009706   0.005041               AGEM -0.001042\n",
       "40   0.030848   0.004813   0.003535               AGEM  0.015966\n",
       "41   0.006986  -0.000695  -0.005842               AGEM -0.001986\n",
       "42  -0.023833   0.020406   0.015013               AGEM -0.016377\n",
       "43  -0.000883   0.025850   0.018390               AGEM  0.001838\n",
       "44   0.031621   0.030056   0.023568               AGEM  0.013625\n",
       "45  -0.002505  -0.003297   0.001657               AGEM  0.004434\n",
       "46   0.010093  -0.010653   0.004044               AGEM  0.004172\n",
       "47   0.015977   0.018654   0.022958               AGEM  0.003526\n",
       "48  -0.003602   0.011838   0.009226               AGEM -0.006044\n",
       "49   0.010555   0.002937   0.026020               AGEM -0.004604\n",
       "50   0.000999   0.004510   0.005656  CVAE-within-Gibbs -0.009517\n",
       "51  -0.000529  -0.002978  -0.003715  CVAE-within-Gibbs  0.004325\n",
       "52  -0.000115  -0.002517  -0.003335  CVAE-within-Gibbs  0.008074\n",
       "53  -0.000236   0.001800   0.002693  CVAE-within-Gibbs -0.014607\n",
       "54  -0.000452  -0.002632  -0.003337  CVAE-within-Gibbs  0.005725\n",
       "55   0.002497   0.005925   0.006849  CVAE-within-Gibbs -0.001085\n",
       "56   0.000011  -0.000897  -0.001330  CVAE-within-Gibbs  0.007790\n",
       "57  -0.000488  -0.001826  -0.002207  CVAE-within-Gibbs  0.001461\n",
       "58   0.003360   0.006606   0.007327  CVAE-within-Gibbs  0.005319\n",
       "59   0.002615   0.005246   0.005975  CVAE-within-Gibbs -0.001234\n",
       "60   0.001486   0.004863   0.005861  CVAE-within-Gibbs -0.004888\n",
       "61   0.000549   0.002311   0.002959  CVAE-within-Gibbs -0.007508\n",
       "62  -0.001790  -0.002999  -0.002998  CVAE-within-Gibbs -0.012511\n",
       "63   0.003368   0.006597   0.007319  CVAE-within-Gibbs  0.005134\n",
       "64  -0.001063  -0.001470  -0.001510  CVAE-within-Gibbs -0.002911\n",
       "65   0.003826   0.004755   0.004493  CVAE-within-Gibbs  0.020807\n",
       "66   0.001139   0.002845   0.003343  CVAE-within-Gibbs -0.002008\n",
       "67  -0.001230  -0.001166  -0.000793  CVAE-within-Gibbs -0.014186\n",
       "68   0.000458   0.001577   0.001916  CVAE-within-Gibbs -0.001978\n",
       "69   0.001727   0.003247   0.003542  CVAE-within-Gibbs  0.004155\n",
       "70   0.001793   0.002321   0.002320  CVAE-within-Gibbs  0.006029\n",
       "71   0.001183  -0.001162  -0.002223  CVAE-within-Gibbs  0.018531\n",
       "72   0.000786   0.001275   0.001367  CVAE-within-Gibbs  0.001572\n",
       "73   0.002322   0.004470   0.004848  CVAE-within-Gibbs  0.007357\n",
       "74   0.001005  -0.000578  -0.001383  CVAE-within-Gibbs  0.015958\n",
       "75  -0.000121  -0.000357  -0.000391  CVAE-within-Gibbs -0.001029\n",
       "76  -0.001252  -0.001746  -0.001590  CVAE-within-Gibbs -0.011276\n",
       "77   0.002994   0.006225   0.007063  CVAE-within-Gibbs  0.000555\n",
       "78   0.000264   0.001491   0.001864  CVAE-within-Gibbs -0.002350\n",
       "79  -0.001551  -0.000456   0.000237  CVAE-within-Gibbs -0.016656\n",
       "80  -0.002028  -0.003231  -0.003392  CVAE-within-Gibbs -0.006430\n",
       "81   0.001779   0.002507   0.002468  CVAE-within-Gibbs  0.009348\n",
       "82  -0.001806  -0.002828  -0.002926  CVAE-within-Gibbs -0.006956\n",
       "83  -0.000677  -0.000642  -0.000655  CVAE-within-Gibbs  0.000473\n",
       "84  -0.001882  -0.002787  -0.002801  CVAE-within-Gibbs -0.009057\n",
       "85   0.000268   0.001536   0.001958  CVAE-within-Gibbs -0.003753\n",
       "86   0.000977   0.001908   0.002069  CVAE-within-Gibbs  0.003291\n",
       "87  -0.000037  -0.000374  -0.000434  CVAE-within-Gibbs -0.000974\n",
       "88   0.002067   0.002524   0.002393  CVAE-within-Gibbs  0.010410\n",
       "89  -0.001811  -0.003704  -0.004177  CVAE-within-Gibbs -0.001042\n",
       "90   0.000513  -0.002457  -0.003617  CVAE-within-Gibbs  0.015966\n",
       "91   0.000447   0.001264   0.001527  CVAE-within-Gibbs -0.001986\n",
       "92  -0.001761  -0.001092  -0.000517  CVAE-within-Gibbs -0.016377\n",
       "93   0.000845   0.001912   0.002143  CVAE-within-Gibbs  0.001838\n",
       "94   0.005048   0.008910   0.009590  CVAE-within-Gibbs  0.013625\n",
       "95   0.001919   0.003473   0.003772  CVAE-within-Gibbs  0.004434\n",
       "96  -0.000501  -0.002192  -0.002734  CVAE-within-Gibbs  0.004172\n",
       "97   0.001710   0.003582   0.003983  CVAE-within-Gibbs  0.003526\n",
       "98  -0.001981  -0.004139  -0.004549  CVAE-within-Gibbs -0.006044\n",
       "99  -0.000434  -0.001126  -0.001185  CVAE-within-Gibbs -0.004604"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d339cbb-6fb0-4cee-9ba8-fc48c1b27a56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('df.csv')\n",
    "u1=np.array(df[['sigma0.01']])[:50]\n",
    "u3=np.array(df[['sigma0.03']])[:50]\n",
    "u5=np.array(df[['sigma0.05']])[:50]\n",
    "u = np.array(df[['TRUE']])[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "78769469-9e28-4bcd-ac20-a59264afcfc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0037586999874183124"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((u-u.mean())**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "324ecf07-7ec3-49f3-bed8-ebbc22806f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011760322822949493"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((u-u1)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3b2d261-77b3-4bf9-9fdb-51274ab30ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2(u1,u):\n",
    "    u_mean = u.mean()\n",
    "    sst = np.sum((u-u_mean)**2)\n",
    "    ssr = np.sum((u-u1)**2)\n",
    "    r2=1-ssr/sst\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32c3d447-ad17-4105-a811-016c9cc4b822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.1288272174729084, -5.105493991195226, -10.348334586740886)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agem\n",
    "r2(u1,u),r2(u3,u),r2(u5,u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38dd4ea1-6e9e-4fa3-960c-8ec29cf75bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "u1=np.array(df[['sigma0.01']])[50:]\n",
    "u3=np.array(df[['sigma0.03']])[50:]\n",
    "u5=np.array(df[['sigma0.05']])[50:]\n",
    "u = np.array(df[['TRUE']])[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "00a5b294-22e9-4d0a-a4fa-abce278e2074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2007317984042497, 0.04437674908766187, -0.05702215758343465)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2(u1,u),r2(u3,u),r2(u5,u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d559da6e-7ccc-4c7e-ac82-572e3f6f50a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61d4cca-8913-44ec-976e-0e58128445e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fbddae-8a00-4b29-b232-8ec4be620045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443c7f0e-0d26-4444-b775-d8aa99c7a298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "947f23b5-2fa0-41e4-96cf-24891956456e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 506.06x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(font_scale = 1.1)\n",
    "g = sns.lmplot(x=\"sigma0.03\", y=\"TRUE\", hue=\"kind\", data=df, palette=\"Set1\",markers=[\"x\", \"o\"])\n",
    "g = (g.set(xlim=(-0.01, 0.01), ylim=(-0.01, 0.01),xticks=[-0.01, -0.005, 0,0.005,0.01], yticks=[-0.01, -0.005, 0,0.005,0.01]))\n",
    "plt.xlabel(\"$\\hat{u}$\",fontsize = 15)\n",
    "plt.ylabel(\"$u$\",fontsize = 15)\n",
    "plt.savefig('sigma0.03.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66453be-e92d-4ded-874f-29a4ebd52f6c",
   "metadata": {},
   "source": [
    "操作方案：首先找到我们的方法中每一个固定sigma中，20个真实数据集中与真实值的RMSE最小的数据，作为使用我们的方法所得到的反演结果。然后取得，agem对应的数据【每一次的第三个数据集】结果作为使用该方法得到的反演结果。然后再进行画图，希望得到的效果是，所有的点都尽量在一条曲线上。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e8ea34-9714-45cc-ad71-abefb33567c3",
   "metadata": {},
   "source": [
    "问题：为啥我们的方法得到的x反演值都是在0附近，没有扩展到其他的区域，是否说明方法上的问题？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bf581c-a219-4634-a462-4926aa540105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "py3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
